{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageFilter\n",
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import math\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SRDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, crop_size, scaling_factor):\n",
    "        \"\"\"\n",
    "        :参数 data_path: 图片文件夹路径\n",
    "        :参数 crop_size: 高分辨率图像裁剪尺寸  （实际训练时不会用原图进行放大，而是截取原图的一个子块进行放大）\n",
    "        :参数 scaling_factor: 放大比例\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path=data_path\n",
    "        self.crop_size = int(crop_size)\n",
    "        self.scaling_factor = int(scaling_factor)\n",
    "        self.images_path=[]\n",
    "\n",
    "        # 如果是训练，则所有图像必须保持固定的分辨率以此保证能够整除放大比例\n",
    "        # 如果是测试，则不需要对图像的长宽作限定\n",
    "\n",
    "        # 读取图像路径\n",
    "        for name in os.listdir(self.data_path):\n",
    "            self.images_path.append(os.path.join(self.data_path,name))\n",
    "\n",
    "        # 数据处理方式\n",
    "        self.pre_trans=transforms.Compose([\n",
    "                                # transforms.CenterCrop(self.crop_size),\n",
    "                                transforms.RandomCrop(self.crop_size),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomVerticalFlip(p=0.5),\n",
    "                                # transforms.ColorJitter(brightness=0.3, contrast=0.3, hue=0.3),\n",
    "                                ])\n",
    "\n",
    "        self.input_transform = transforms.Compose([\n",
    "                                transforms.Resize(self.crop_size//self.scaling_factor),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5],std=[0.5]),\n",
    "                                ])\n",
    "\n",
    "        self.target_transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5],std=[0.5]),\n",
    "                                ])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 读取图像\n",
    "        img = Image.open(self.images_path[i], mode='r')\n",
    "        img = img.convert('RGB')\n",
    "        img=self.pre_trans(img)\n",
    "\n",
    "        lr_img = self.input_transform(img)\n",
    "        hr_img = self.target_transform(img.copy())\n",
    "        \n",
    "\n",
    "        return lr_img, hr_img\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 是可以保持w，h不变的\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    卷积模块,由卷积层, BN归一化层, 激活层构成.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n",
    "        \"\"\"\n",
    "        :参数 in_channels: 输入通道数\n",
    "        :参数 out_channels: 输出通道数\n",
    "        :参数 kernel_size: 核大小\n",
    "        :参数 stride: 步长\n",
    "        :参数 batch_norm: 是否包含BN层\n",
    "        :参数 activation: 激活层类型; 如果没有则为None\n",
    "        \"\"\"\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "\n",
    "        if activation is not None:\n",
    "            activation = activation.lower()\n",
    "            assert activation in {'prelu', 'leakyrelu', 'tanh'}\n",
    "\n",
    "        # 层列表\n",
    "        layers = list()\n",
    "\n",
    "        # 1个卷积层\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=kernel_size // 2))\n",
    "\n",
    "        # 1个BN归一化层\n",
    "        if batch_norm is True:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "\n",
    "        # 1个激活层\n",
    "        if activation == 'prelu':\n",
    "            layers.append(nn.PReLU())\n",
    "        elif activation == 'leakyrelu':\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        elif activation == 'tanh':\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # 合并层\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv_block(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# w,h放大\n",
    "class SubPixelConvolutionalBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size=3, n_channels=64, scaling_factor=2):\n",
    "        super(SubPixelConvolutionalBlock, self).__init__()\n",
    "\n",
    "        # 首先通过卷积将通道数扩展为 scaling factor^2 倍\n",
    "        self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels * (scaling_factor ** 2),\n",
    "                              kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "\n",
    "        # 进行像素清洗，合并相关通道数据 放大了图像\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n",
    "        # 最后添加激活层\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.conv(input)\n",
    "        output = self.pixel_shuffle(output)  \n",
    "        output = self.prelu(output) \n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    残差模块, 包含两个卷积模块和一个跳连.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=3, n_channels=64):\n",
    "        \"\"\"\n",
    "        :参数 kernel_size: 核大小\n",
    "        :参数 n_channels: 输入和输出通道数（由于是ResNet网络，需要做跳连，因此输入和输出通道数是一致的）\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # 第一个卷积块\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation='PReLu')\n",
    "\n",
    "        # 第二个卷积块\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        前向传播.\n",
    "\n",
    "        :参数 input: 输入图像集，张量表示，大小为 (N, n_channels, w, h)\n",
    "        :返回: 输出图像集，张量表示，大小为 (N, n_channels, w, h)\n",
    "        \"\"\"\n",
    "        residual = input  # (N, n_channels, w, h)\n",
    "        output = self.conv_block1(input)  # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
    "        output = output + residual  # (N, n_channels, w, h)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SRResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=2):\n",
    "        \"\"\"\n",
    "        :参数 large_kernel_size: 第一层卷积和最后一层卷积核大小\n",
    "        :参数 small_kernel_size: 中间层卷积核大小\n",
    "        :参数 n_channels: 中间层通道数\n",
    "        :参数 n_blocks: 残差模块数\n",
    "        :参数 scaling_factor: 放大比例\n",
    "        \"\"\"\n",
    "        super(SRResNet, self).__init__()\n",
    "\n",
    "        # 放大比例必须为 2、 4 或 8\n",
    "        scaling_factor = int(scaling_factor)\n",
    "        assert scaling_factor in {2, 4, 8}, \"放大比例必须为 2、 4 或 8!\"\n",
    "\n",
    "        # 第一个卷积块\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='PReLu')\n",
    "\n",
    "\n",
    "        # 一系列残差模块, 每个残差模块包含一个跳连接\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(kernel_size=small_kernel_size, n_channels=n_channels) for i in range(n_blocks)])\n",
    "\n",
    "\n",
    "        # 第二个卷积块\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels,\n",
    "                                              kernel_size=small_kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "        # 放大通过子像素卷积模块实现, 每个模块放大两倍 log2 2=1\n",
    "        n_subpixel_convolution_blocks = int(math.log2(scaling_factor))\n",
    "        self.subpixel_convolutional_blocks = nn.Sequential(\n",
    "            *[SubPixelConvolutionalBlock(kernel_size=small_kernel_size, n_channels=n_channels, scaling_factor=2) for i\n",
    "              in range(n_subpixel_convolution_blocks)])\n",
    "\n",
    "        # 最后一个卷积模块\n",
    "        self.conv_block3 = ConvolutionalBlock(in_channels=n_channels, out_channels=3, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='Tanh')\n",
    "\n",
    "    def forward(self, lr_imgs):\n",
    "        \"\"\"\n",
    "        :参数 lr_imgs: 低分辨率输入图像集, 张量表示，大小为 (N, 3, w, h)\n",
    "        :返回: 高分辨率输出图像集, 张量表示， 大小为 (N, 3, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        output = self.conv_block1(lr_imgs)  # (16, 3, 24, 24)\n",
    "        residual = output  # (16, 64, 24, 24)\n",
    "        output = self.residual_blocks(output)  # (16, 64, 24, 24)\n",
    "        output = self.conv_block2(output)  # (16, 64, 24, 24)\n",
    "        output = output + residual  # (16, 64, 24, 24)\n",
    "        output = self.subpixel_convolutional_blocks(output)  # (16, 64, 24 * 2, 24 * 2)\n",
    "        sr_imgs = self.conv_block3(output)  # (16, 3, 24 * 2, 24 * 2)\n",
    "\n",
    "        return sr_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='G:/DIV2K/DIV2K/DIV2K_train_HR'\n",
    "test_path='../../SRCNN/SRCNN-WBQ/data/test'\n",
    "\n",
    "crop_size = 224      # 高分辨率图像裁剪尺寸\n",
    "scaling_factor = 2  # 放大比例\n",
    "\n",
    "# 模型参数\n",
    "large_kernel_size = 9   # 第一层卷积和最后一层卷积的核大小\n",
    "small_kernel_size = 3   # 中间层卷积的核大小\n",
    "n_channels = 64         # 中间层通道数\n",
    "n_blocks = 16           # 残差模块数量\n",
    "\n",
    "# 学习参数\n",
    "checkpoint = '../model/rsresnet.pth'   # 预训练模型路径，如果不存在则为None\n",
    "batch_size = 30    # 批大小\n",
    "start_epoch = 1     # 轮数起始位置\n",
    "epochs = 300        # 迭代轮数\n",
    "workers = 1        # 工作线程数\n",
    "lr = 1e-3           # 学习率\n",
    "\n",
    "# 先前的psnr\n",
    "pre_psnr=0\n",
    "\n",
    "# 设备参数\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "\n",
    "ngpu = 1\n",
    "\n",
    "# cudnn.benchmark = True # 对卷积进行加速\n",
    "\n",
    "# writer = SummaryWriter('../log') # 实时监控     使用命令 tensorboard --logdir runs  进行查看\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint):\n",
    "    model = torch.load(checkpoint).to(device)\n",
    "    print('加载先前模型成功')\n",
    "else:\n",
    "    print('未加载原有模型训练')\n",
    "    model = SRResNet(large_kernel_size=large_kernel_size,\n",
    "                    small_kernel_size=small_kernel_size,\n",
    "                    n_channels=n_channels,\n",
    "                    n_blocks=n_blocks,\n",
    "                    scaling_factor=scaling_factor)\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "train_dataset = SRDataset(train_path, crop_size, scaling_factor)\n",
    "test_dataset = SRDataset(test_path, crop_size, scaling_factor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True) \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True)\n",
    "\n",
    "    # 开始逐轮训练\n",
    "for epoch in range(start_epoch, epochs+1):\n",
    "\n",
    "    model.train()  # 训练模式：允许使用批样本归一化\n",
    "    train_loss=0\n",
    "    n_iter = len(train_loader)\n",
    "\n",
    "    # 按批处理\n",
    "    for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n",
    "        lr_imgs = lr_imgs.to(device)\n",
    "        hr_imgs = hr_imgs.to(device)\n",
    "\n",
    "        sr_imgs = model(lr_imgs)\n",
    "        loss = criterion(sr_imgs, hr_imgs)  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        # print('ga')\n",
    "\n",
    "    epoch_loss_train=train_loss / n_iter\n",
    "\n",
    "    # tensorboard\n",
    "    # writer.add_scalar('SRResNet/MSE_Loss_train', epoch_loss_train, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch}. Training loss: {epoch_loss_train}\")\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()  # 测试模式\n",
    "    test_loss=0\n",
    "    all_psnr = 0\n",
    "    n_iter = len(test_loader)\n",
    "    # print(n_iter)\n",
    "    # print(len(test_dataset))\n",
    "    with torch.no_grad():\n",
    "        for i, (lr_imgs, hr_imgs) in enumerate(test_loader):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            hr_imgs = hr_imgs.to(device)\n",
    "\n",
    "            sr_imgs = model(lr_imgs)\n",
    "            loss = criterion(sr_imgs, hr_imgs)\n",
    "\n",
    "            psnr = 10 * log10(1 / loss.item())\n",
    "            all_psnr+=psnr\n",
    "            test_loss+=loss.item()\n",
    "    \n",
    "    epoch_loss_test=test_loss/n_iter\n",
    "    epoch_psnr=all_psnr / n_iter\n",
    "\n",
    "    # tensorboard\n",
    "    # writer.add_scalar('SRResNet/MSE_Loss_test', epoch_loss_test, epoch)\n",
    "    # writer.add_scalar('SRResNet/test_psnr', epoch_psnr, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch}. Testing loss: {epoch_loss_test}\")\n",
    "    print(f\"Average PSNR: {epoch_psnr} dB.\")\n",
    "\n",
    "    if epoch_psnr>pre_psnr:\n",
    "        torch.save(model, checkpoint)\n",
    "        pre_psnr=epoch_psnr\n",
    "        print('模型更新成功')\n",
    "\n",
    "    scheduler.step()\n",
    "    # 打印当前学习率\n",
    "    print('当前学习率为：',end=' ')\n",
    "    print(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "    print('*'*50)\n",
    "# 训练结束关闭监控\n",
    "# writer.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06ad926a62d88629ca0e7de749b0cd09dfb9c2f00a43ec80a168339033375a01"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
